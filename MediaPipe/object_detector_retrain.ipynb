{"cells":[{"cell_type":"markdown","metadata":{"id":"eOlNZgOafs5b"},"source":["Project: /mediapipe/_project.yaml\n","Book: /mediapipe/_book.yaml\n","\n","<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n","\n","# Object detection model customization guide\n","\n","<table align=\"left\" class=\"buttons\">\n","  <td>\n","    <a href=\"https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/object_detector.ipynb\" target=\"_blank\">\n","      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png\" alt=\"Colab logo\"> Run in Colab\n","    </a>\n","  </td>\n","\n","  <td>\n","    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/object_detector.ipynb\" target=\"_blank\">\n","      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n","      View on GitHub\n","    </a>\n","  </td>\n","</table>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wGYgvC7P7faD"},"outputs":[],"source":["#@title License information\n","# Copyright 2023 The MediaPipe Authors.\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","#\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMSCcHh1LScM"},"outputs":[],"source":["!python --version\n","!pip install --upgrade pip\n","!pip install mediapipe-model-maker"]},{"cell_type":"markdown","metadata":{"id":"7NXvZgLPLh6n"},"source":["Gerekli Python sınıflarını içe aktarmak için aşağıdaki kodu kullanabilirsiniz:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oazmbPzKHYFq"},"outputs":[],"source":["from google.colab import files\n","import os\n","import json\n","import tensorflow as tf\n","assert tf.__version__.startswith('2')\n","from mediapipe_model_maker import object_detector"]},{"cell_type":"markdown","metadata":{"id":"_zP1AkaRL72Z"},"source":["## Veri hazırlama"]},{"cell_type":"markdown","metadata":{"id":"EVnSV0ZQMA5-"},"source":["Bir nesne algılama modelini yeniden eğitmek için, tamamlanmış modelin tanıyabilmesi gereken öğeleri veya sınıfları içeren bir veri kümesine ihtiyaç vardır. Bunun için halka açık bir veri kümesini kendi kullanım durumunuza uygun sınıflarla sınırlayarak, kendi veri kümenizi oluşturarak veya her ikisini birleştirerek yapabilirsiniz. Veri kümesi, yeni bir modeli eğitmek için gereken veri kümesinden çok daha küçük olabilir. Örneğin, birçok referans modelini eğitmek için kullanılan COCO (https://cocodataset.org/) veri kümesi, 91 farklı nesne sınıfını içeren yüz binlerce görüntü içerir. Model Maker ile transfer öğrenme kullanarak mevcut bir modeli daha küçük bir veri kümesiyle yeniden eğitebilir ve çıkarım doğruluk hedeflerinize bağlı olarak iyi performans elde edebilirsiniz. Bu talimatlar, 2 farklı Android figürü veya 2 sınıf içeren ve toplamda 62 eğitim görüntüsü bulunan daha küçük bir örnek veri kümesini kullanır.\n","\n","Örnek veri kümesini indirmek için aşağıdaki kodu kullanabilirsiniz:\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"C3VLcihz2s-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mz3-eHe07FEX"},"outputs":[],"source":["!wget https://storage.googleapis.com/mediapipe-tasks/object_detector/android_figurine.zip\n","!unzip android_figurine.zip\n","train_dataset_path = \"android_figurine/train\"\n","validation_dataset_path = \"android_figurine/validation\""]},{"cell_type":"markdown","metadata":{"id":"oEVcacUj7L1l"},"source":["Bu kod, veri kümesini android_figurine klasör konumunda saklar. Klasör, sırasıyla android_figurine/train ve android_figurine/validation olarak bulunan eğitim ve doğrulama veri kümeleri için iki alt klasör içerir. Eğitim ve doğrulama veri kümeleri, aşağıda açıklanan COCO Veri Seti formatını izler:"]},{"cell_type":"markdown","metadata":{"id":"vbIAPjPUMHiK"},"source":["### Desteklenen veriseti formatları\n","Model Maker Nesne Algılama API'si aşağıdaki veri kümesi formatlarını okuma desteği sunar:\n","\n","#### COCO formatı\n","COCO veri kümesi formatı, tüm görüntüleri depolayan bir data klasörü ve tüm görüntülerin nesne etiketlerini içeren tek bir labels.json dosyasına sahiptir.\n","```\n","<dataset_dir>/\n","  data/\n","    <img0>.<jpg/jpeg>\n","    <img1>.<jpg/jpeg>\n","    ...\n","  labels.json\n","```\n","where `labels.json` is formatted as:\n","```\n","{\n","  \"categories\":[\n","    {\"id\":1, \"name\":<cat1_name>},\n","    ...\n","  ],\n","  \"images\":[\n","    {\"id\":0, \"file_name\":\"<img0>.<jpg/jpeg>\"},\n","    ...\n","  ],\n","  \"annotations\":[\n","    {\"id\":0, \"image_id\":0, \"category_id\":1, \"bbox\":[x-top left, y-top left, width, height]},\n","    ...\n","  ]\n","}\n","```\n","\n","#### PASCAL VOC formatı\n","PASCAL VOC veri kümesi formatı ayrıca tüm görüntüleri depolayan bir data klasörü içerir, ancak etiketlemeler her görüntü için karşılık gelen XML dosyalarına Annotations klasöründe bölünmüştür. Bu düzenleme, her görüntünün ayrı ayrı etiketlenmesini ve bu etiketlemelerin ayrı XML dosyalarında saklanmasını sağlar. Bu şekilde veriler daha düzenli ve yönetilebilir hale gelir.\n","```\n","<dataset_dir>/\n","  data/\n","    <file0>.<jpg/jpeg>\n","    ...\n","  Annotations/\n","    <file0>.xml\n","    ...\n","```\n","where the xml files are formatted as:\n","```\n","<annotation>\n","  <filename>file0.jpg</filename>\n","  <object>\n","    <name>kangaroo</name>\n","    <bndbox>\n","      <xmin>233</xmin>\n","      <ymin>89</ymin>\n","      <xmax>386</xmax>\n","      <ymax>262</ymax>\n","    </bndbox>\n","  </object>\n","  <object>\n","    ...\n","  </object>\n","  ...\n","</annotation>\n","```"]},{"cell_type":"markdown","metadata":{"id":"G7TPn8Mb_aJb"},"source":["### Review dataset"]},{"cell_type":"markdown","metadata":{"id":"L64U7mgPNKec"},"source":["Veri kümesi içeriğini, labels.json dosyasındaki kategorileri yazdırarak doğrulayabilirsiniz. Toplamda 3 kategori olmalıdır. 0 endeksi her zaman arka plan sınıfı olarak ayarlanır ve veri kümesinde kullanılmayabilir. android ve pig_android adında iki arka plan olmayan kategori olmalıdır."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2f_Z-TAwNK3n"},"outputs":[],"source":["with open(os.path.join(train_dataset_path, \"labels.json\"), \"r\") as f:\n","  labels_json = json.load(f)\n","for category_item in labels_json[\"categories\"]:\n","  print(f\"{category_item['id']}: {category_item['name']}\")"]},{"cell_type":"markdown","metadata":{"id":"xr-7QJ05PmyS"},"source":["Veri kümesini daha iyi anlamak için birkaç örnek görseli sınırlayıcı kutularıyla birlikte çizin."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kTw3uodPl7-"},"outputs":[],"source":["\n","import matplotlib.pyplot as plt\n","from matplotlib import patches, text, patheffects\n","from collections import defaultdict\n","import math\n","\n","import matplotlib.image as mpimg\n","from matplotlib.pyplot import imshow\n","\n","%matplotlib inline\n","def draw_outline(obj):\n","  obj.set_path_effects([patheffects.Stroke(linewidth=4,  foreground='black'), patheffects.Normal()])\n","def draw_box(ax, bb):\n","  patch = ax.add_patch(patches.Rectangle((bb[0],bb[1]), bb[2], bb[3], fill=False, edgecolor='red', lw=2))\n","  draw_outline(patch)\n","def draw_text(ax, bb, txt, disp):\n","  text = ax.text(bb[0],(bb[1]-disp),txt,verticalalignment='top'\n","  ,color='white',fontsize=10,weight='bold')\n","  draw_outline(text)\n","def draw_bbox(ax, annotations_list, id_to_label, image_shape):\n","  for annotation in annotations_list:\n","    cat_id = annotation[\"category_id\"]\n","    bbox = annotation[\"bbox\"]\n","    draw_box(ax, bbox)\n","    draw_text(ax, bbox, id_to_label[cat_id], image_shape[0] * 0.05)\n","def visualize(dataset_folder, max_examples=None):\n","  with open(os.path.join(dataset_folder, \"labels.json\"), \"r\") as f:\n","    labels_json = json.load(f)\n","\n","  images = labels_json[\"images\"]\n","\n","  cat_id_to_label = {item[\"id\"]:item[\"name\"] for item in labels_json[\"categories\"]}\n","  image_annots = defaultdict(list)\n","  for annotation_obj in labels_json[\"annotations\"]:\n","    image_id = annotation_obj[\"image_id\"]\n","    image_annots[image_id].append(annotation_obj)\n","\n","  if max_examples is None:\n","    max_examples = len(image_annots.items())\n","  n_rows = math.ceil(max_examples / 3)\n","  fig, axs = plt.subplots(n_rows, 3, figsize=(24, n_rows*8)) # 3 columns(2nd index), 8x8 for each image\n","  for ind, (image_id, annotations_list) in enumerate(list(image_annots.items())[:max_examples]):\n","    ax = axs[ind//3, ind%3]\n","    img = plt.imread(os.path.join(dataset_folder, \"images\", images[image_id][\"file_name\"]))\n","    ax.imshow(img)\n","    draw_bbox(ax, annotations_list, cat_id_to_label, img.shape)\n","    #imshow(img)\n","  plt.show()\n","\n","visualize(train_dataset_path,9)"]},{"cell_type":"markdown","metadata":{"id":"ANqfl-ghQM74"},"source":["### Veri kümesi oluştur\n","Veri Kümesi sınıfı, COCO veya PASCAL VOC veri kümelerini yüklemek için iki yönteme sahiptir:\n","\n","* Dataset.from_coco_folder\n","* Dataset.from_pascal_voc_folder\n","\n","android_figurines veri kümesi COCO veri kümesi formatında olduğundan, veri kümesini train_dataset_path ve validation_dataset_path konumlarındaki veriyi yüklemek için from_coco_folder yöntemini kullanmalısınız. Veri kümesini yüklerken, veriler sağlanan path üzerinden ayrıştırılacak ve daha sonra kullanılmak üzere önbelleğe alınacak olan standartlaştırılmış bir TFRecord (https://www.tensorflow.org/tutorials/load_data/tfrecord) formatına dönüştürülür. Aynı veri kümesinin birden fazla önbelleğini kaydetmekten kaçınmak için bir cache_dir konumu oluşturmalı ve tüm eğitimleriniz için bunu yeniden kullanmalısınız."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOdyImqyI6s-"},"outputs":[],"source":["train_data = object_detector.Dataset.from_coco_folder(train_dataset_path, cache_dir=\"/tmp/od_data/train\")\n","validation_data = object_detector.Dataset.from_coco_folder(validation_dataset_path, cache_dir=\"/tmp/od_data/validation\")\n","print(\"train_data size: \", train_data.size)\n","print(\"validation_data size: \", validation_data.size)"]},{"cell_type":"markdown","metadata":{"id":"det3wYoWRRDs"},"source":["## Retrain model\n","\n","Verilerinizi hazırlamayı tamamladığınızda, eğitim verilerinizle tanımlanan yeni nesneleri veya sınıfları tanımak için bir modeli yeniden eğitmeye başlayabilirsiniz. Aşağıdaki talimatlar, önceki bölümde hazırlanan verileri kullanarak iki tür Android figürünü tanımak için bir görüntü sınıflandırma modelini yeniden eğitmek için kullanılır."]},{"cell_type":"markdown","metadata":{"id":"f9AzF_CGA7mj"},"source":["### Set retraining options\n","\n","There are a few required settings to run retraining aside from your training dataset: output directory for the model, and the model architecture. Use `HParams` to specify the `export_dir` parameter for the output directory. Use the `SupportedModels` class to specify the model architecture. The object detector solution supports the following model architectures:\n","* `MobileNet-V2`\n","* `MobileNet-MultiHW-AVG`\n","\n","For more advanced customization of training parameters, see the  [Hyperparameters](#hyperparameters) section below.\n","\n","To set the required parameters, use the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZHjWHM1JyiN"},"outputs":[],"source":["#spec = object_detector.SupportedModels.MOBILENET_MULTI_AVG\n","spec = object_detector.SupportedModels.MOBILENET_V2\n","\n","hparams = object_detector.HParams(export_dir='exported_model')\n","options = object_detector.ObjectDetectorOptions(\n","    supported_model=spec,\n","    hparams=hparams\n",")"]},{"cell_type":"markdown","metadata":{"id":"t3Kto1RlCcPj"},"source":["### Run retraining\n","Eğitim veri kümeniz ve yeniden eğitim seçenekleriniz hazır olduğunda, yeniden eğitim sürecini başlatmaya hazırsınız demektir. Bu süreç, mevcut hesaplama kaynaklarınıza bağlı olarak birkaç dakikadan birkaç saate kadar sürebilen kaynak yoğun bir süreçtir. Standart GPU çalışma zamanlarına sahip bir Google Colab ortamı kullanarak, aşağıdaki örnek yeniden eğitim yaklaşık olarak 2 ila 4 dakika sürer.\n","\n","Yeniden eğitim sürecini başlatmak için daha önce tanımladığınız veri kümesi ve seçenekleri kullanarak create() yöntemini kullanın:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5bIsWBZCb8d"},"outputs":[],"source":["model = object_detector.ObjectDetector.create(\n","    train_data=train_data,\n","    validation_data=validation_data,\n","    options=options)"]},{"cell_type":"markdown","metadata":{"id":"RuRapoFiRp34"},"source":["### Evaluate the model performance\n","\n","After training the model, evaluate it on validation dataset and print the loss and coco_metrics. The most important metric for evaluating the model performance is typically the \"AP\" coco metric for Average Precision."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJvB_nf7RwzJ"},"outputs":[],"source":["loss, coco_metrics = model.evaluate(validation_data, batch_size=4)\n","print(f\"Validation loss: {loss}\")\n","print(f\"Validation coco metrics: {coco_metrics}\")"]},{"cell_type":"markdown","metadata":{"id":"ax8TkYA9VJUv"},"source":["## Export model\n","\n","After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label map."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWZqnGEKVP13"},"outputs":[],"source":["model.export_model()\n","!ls exported_model\n","files.download('exported_model/model.tflite')"]},{"cell_type":"markdown","metadata":{"id":"UcYu5ENbT4T6"},"source":["## Model quantization"]},{"cell_type":"markdown","metadata":{"id":"nIeJjCfWTnBj"},"source":["Model quantization is a model modification technique that can reduce the model size and improve the speed of predictions with only a relatively minor decrease in accuracy.\n","\n","This section of the guide explains how to apply quantization to your model. Model Maker supports two forms of quantization for object detector:\n","1. Quantization Aware Training: 8 bit integer precision for CPU usage\n","2. Post-Training Quantization: 16 bit floating point precision for GPU usage"]},{"cell_type":"markdown","metadata":{"id":"UcB3DRfHWDfs"},"source":["### Quantization aware training (int8 quantization)\n","Quantization aware training (QAT) is a fine-tuning step which happens after fully training your model. This technique further tunes a model which emulates inference time quantization in order to account for the lower precision of 8 bit integer quantization. For on-device applications with a standard CPU, use Int8 precision. For more information, see the [TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/training) documentation.\n","\n","To apply quantization aware training and export to an int8 model, create a `QATHParams` configuration and run the `quantization_aware_training` method. See the **Hyperparameters** section below on detailed usage of `QATHParams`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_7nRSQT9WCS-"},"outputs":[],"source":["qat_hparams = object_detector.QATHParams(learning_rate=0.3, batch_size=4, epochs=10, decay_steps=6, decay_rate=0.96)\n","model.quantization_aware_training(train_data, validation_data, qat_hparams=qat_hparams)\n","qat_loss, qat_coco_metrics = model.evaluate(validation_data)\n","print(f\"QAT validation loss: {qat_loss}\")\n","print(f\"QAT validation coco metrics: {qat_coco_metrics}\")"]},{"cell_type":"markdown","metadata":{"id":"rT7grgHOW048"},"source":["The QAT step often requires multiple runs to tune the parameters of training. To avoid having to rerun model training using the `create` method, use the `restore_float_ckpt` method to restore the model state back to the fully trained float model(After running the `create` method) in order to run QAT again."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHDPWbIaXjR4"},"outputs":[],"source":["new_qat_hparams = object_detector.QATHParams(learning_rate=0.9, batch_size=4, epochs=15, decay_steps=5, decay_rate=0.96)\n","model.restore_float_ckpt()\n","model.quantization_aware_training(train_data, validation_data, qat_hparams=new_qat_hparams)\n","qat_loss, qat_coco_metrics = model.evaluate(validation_data)\n","print(f\"QAT validation loss: {qat_loss}\")\n","print(f\"QAT validation coco metrics: {qat_coco_metrics}\")"]},{"cell_type":"markdown","metadata":{"id":"AfWo6TVpWJfr"},"source":["Finally, us the `export_model` to export to an int8 quantized model. The `export_model` function will automatically export to either float32 or int8 model depending on whether `quantization_aware_training` was run."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsixePxCWJDp"},"outputs":[],"source":["model.export_model('model_int8_qat.tflite')\n","!ls -lh exported_model\n","files.download('exported_model/model_int8_qat.tflite')"]},{"cell_type":"markdown","metadata":{"id":"eO8nZR3Cgx8_"},"source":["### Post-training quantization (fp16 quantization)\n","\n","Post-training model quantization is a model modification technique that can reduce the model size and improve the speed of predictions with only a relatively minor decrease in accuracy. This approach reduces the size of the data processed by the model, for example by transforming 32-bit floating point numbers to 16-bit floats. Float16 quantization is reccomended for GPU usage. For more information, see the [TensorFlow Lite](https://www.tensorflow.org/model_optimization/guide/quantization/post_training) documentation.\n","\n","First, import the MediaPipe Model Maker quantization module:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yo7cQ_N-ZE8A"},"outputs":[],"source":["from mediapipe_model_maker import quantization"]},{"cell_type":"markdown","metadata":{"id":"OE8j5cloZTo-"},"source":["Define a QuantizationConfig object using the `for_float16()` class method. This configuration modifies a trained model to use 16-bit floating point numbers instead of 32-bit floating point numbers. You can further customize the quantization process by setting additional parameters for the QuantizationConfig class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbwc3g_Fa3dv"},"outputs":[],"source":["quantization_config = quantization.QuantizationConfig.for_float16()"]},{"cell_type":"markdown","metadata":{"id":"qTrkDXi8bM_L"},"source":["Export the model using the additional quantization_config object to apply post-training quantization. Note that if you previously ran `quantization_aware_training`, you must first convert the model back to a float model by using `restore_float_ckpt`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmzEu_AjbMPI"},"outputs":[],"source":["model.restore_float_ckpt()\n","model.export_model(model_name=\"model_fp16.tflite\", quantization_config=quantization_config)\n","!ls -lh exported_model\n","files.download('exported_model/model_fp16.tflite')"]},{"cell_type":"markdown","metadata":{"id":"npaRBUB3ZevY"},"source":["## Hyperparameters\n","You can further customize the model using the ObjectDetectorOptions class, which has three parameters for `SupportedModels`, `ModelOptions`, and `HParams`.\n","\n","Use the `SupportedModels` enum class to specify the model architecture to use for training. The following model architectures are supported:\n","* MOBILENET_V2\n","* MOBILENET_V2_I320\n","* MOBILENET_MULTI_AVG\n","* MOBILENET_MULTI_AVG_I384\n","\n","Use the `HParams` class to customize other parameters related to training and saving the model:\n","* `learning_rate`: Learning rate to use for gradient descent training. Defaults to 0.3.\n","* `batch_size`: Batch size for training. Defaults to 8.\n","* `epochs`: Number of training iterations over the dataset. Defaults to 30.\n","* `cosine_decay_epochs`: The number of epochs for cosine decay learning rate. See [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay) for more info. Defaults to None, which is equivalent to setting it to `epochs`.\n","* `cosine_decay_alpha`: The alpha value for cosine decay learning rate. See [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay) for more info. Defaults to 1.0, which means no cosine decay.\n","\n","Use the `ModelOptions` class to customize parameters related to the model itself:\n","* `l2_weight_decay`: L2 regularization penalty used in [tf.keras.regularizers.L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2). Defaults to 3e-5.\n","\n","Uset the `QATHParams` class to customize training parameters for Quantization Aware Training:\n","* `learning_rate`: Learning rate to use for gradient descent QAT. Defaults to 0.3.\n","* `batch_size`: Batch size for QAT. Defaults to 8\n","* `epochs`: Number of training iterations over the dataset. Defaults to 15.\n","* `decay_steps`: Learning rate decay steps for Exponential Decay. See [tf.keras.optimizers.schedules.ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) for more information. Defaults to 8\n","* `decay_rate`: Learning rate decay rate for Exponential Decay. See [tf.keras.optimizers.schedules.ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) for more information. Defaults to 0.96."]},{"cell_type":"markdown","metadata":{"id":"9HCrUl8z6liX"},"source":["## Benchmarking\n","Below is a summary of our benchmarking results for the supported model architectures. These models were trained and evaluated on the same android figurines dataset as this notebook. When considering the model benchmarking results, there are a few important caveats to keep in mind:\n","* The android figurines dataset is a small and simple dataset with 62 training examples and 10 validation examples. Since the dataset is quite small, metrics may vary drastically due to variances in the training process. This dataset was provided for demo purposes and it is recommended to collect more data samples for better performing models.\n","* The float32 models were trained with the default HParams, and the QAT step for the int8 models was run with `QATHParams(learning_rate=0.1, batch_size=4, epochs=30, decay_rate=1)`.\n","* For your own dataset, you will likely need to tune values for both HParams and QATHParams in order to achieve the best results. See the [Hyperparameters](#hyperparameters) section above for more information on configuring training parameters.\n","* All latency numbers are benchmarked on the Pixel 6.\n","\n","\n","<table>\n","<thead>\n","<col>\n","<col>\n","<colgroup span=\"2\"></colgroup>\n","<colgroup span=\"2\"></colgroup>\n","<colgroup span=\"2\"></colgroup>\n","<tr>\n","<th rowspan=\"2\">Model architecture</th>\n","<th rowspan=\"2\">Input Image Size</th>\n","<th colspan=\"2\" scope=\"colgroup\">Test AP</th>\n","<th colspan=\"2\" scope=\"colgroup\">CPU Latency</th>\n","<th colspan=\"2\" scope=\"colgroup\">Model Size</th>\n","</tr>\n","<tr>\n","<th>float32</th>\n","<th>QAT int8</th>\n","<th>float32</th>\n","<th>QAT int8</th>\n","<th>float32</th>\n","<th>QAT int8</th>\n","</tr>\n","</thead>\n","<tbody>\n","<tr>\n","<td>MobileNetV2</td>\n","<td>256x256</td>\n","<td>88.4%</td>\n","<td>73.5%</td>\n","<td>48ms</td>\n","<td>16ms</td>\n","<td>11MB</td>\n","<td>3.2MB</td>\n","</tr>\n","<tr>\n","<td>MobileNetV2 I320</td>\n","<td>320x320</td>\n","<td>89.1%</td>\n","<td>75.5%</td>\n","<td>75ms</td>\n","<td>33.38ms</td>\n","<td>10MB</td>\n","<td>3.3MB</td>\n","</tr>\n","<tr>\n","<td>MobileNet MultiHW AVG</td>\n","<td>256x256</td>\n","<td>88.5%</td>\n","<td>70.0%</td>\n","<td>56ms</td>\n","<td>19ms</td>\n","<td>13MB</td>\n","<td>3.6MB</td>\n","</tr>\n","<tr>\n","<td>MobileNet MultiHW AVG I384</td>\n","<td>384x384</td>\n","<td>92.7%</td>\n","<td>73.4%</td>\n","<td>238ms</td>\n","<td>41ms</td>\n","<td>13MB</td>\n","<td>3.6MB</td>\n","</tr>\n","\n","</tbody>\n","</table>\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOCPzKohXxy6"},"outputs":[],"source":[]}],"metadata":{"colab":{"last_runtime":{"build_target":"//learning/grp/tools/ml_python:ml_notebook","kind":"private"},"private_outputs":true,"provenance":[{"file_id":"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/object_detector.ipynb","timestamp":1696163845903},{"file_id":"11PG1YgsQWWLJ8jpqJ6QY7hjYWzxVwoCb","timestamp":1677706798050}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}